# Evaluating The Cognitive Plausibility of Transformer-based Models: Predicting Articulation Rate from Surprisal Estimates

Surprisal is a measure of the amount of information carried by a linguistic unit (e.g., a word), which is inversely related to its predictability: high predictable words have low-information content and unexpected words carry more information. Research has shown a negative relationship between surprisal and speech tempo—such as articulation rate (AR)—as reflected in human behavioral data, including reading times. Transformer-based Large Language Models (LLMs) like GPT-2 produce surprisal estimates that are strongly predictive of reading times in humans, effectively capturing this negative relationship. However, recent analyses have observed a positiverelationship between the Psychometric Predictive Power (PPP) of larger GPT-2 models and reading times—compared to smaller GPT-2 variants—suggesting that larger models poorly approximate human reading behavior. However, it remains unexplored whether surprisal estimates from BERT can better predict AR in read speech than GPT-2. The aim of this work is to evaluate which of GPT-2 and BERT are better cognitive models of language processing in humans. We employ General Linear Models (GLMs) to model AR as a function of surprisal estimates from both models, while controlling for word position, word length, and final lengthening. The results suggest that BERT is a more cognitively plausible model of human language processing than GPT-2, based on two observations. Firstly, BERT models trained with smaller context window sizes obtained surprisal estimates that led to improved PPP. Secondly, the smallest BERT model—based on a context window size of 128 tokens–achieved the lowest Akaike Information Criterion (AIC) score, suggesting a better fit to the observed data. We argue that our result have broader implications for cognitive modeling, especially the possibility of BERT to capture different psycholinguistic processes in humans than GPT-2—language planning, retrospective simulation, and the interaction between prediction and comprehension. Our results question prior assumptions that BERT is a cognitively implausible model. 

## Pipeline
The following pipeline was built to pre-process speech recordings of the King James Version Bible from the [Faith Comes by Hearing](https://www.faithcomesbyhearing.com/) database. The chosen collection corresponds to the ID-code ENGKJV, from which 28 New Testament books are contained. The entire collection needs to be downloaded manually from the recording [database](https://www.faithcomesbyhearing.com/audio-bible-resources/mp3-downloads). Create a root folder `ENGKJV` with the subfolders `audio` for storing the 260 audio recorings in mp3, `text` for storing the orthographies, and `text_grids` for storing the outputs of the forced aligner.

